"""
Advanced caching and memoization system for photonic compilation.
Generation 3: High-performance caching with intelligent invalidation and compression.
"""

import time
import threading
import hashlib
import pickle
import zlib
import json
from typing import Dict, Any, Optional, List, Tuple, Callable, Union
from dataclasses import dataclass, field
from enum import Enum
from abc import ABC, abstractmethod
from pathlib import Path
import logging
import weakref
from collections import OrderedDict, defaultdict
import asyncio
import aiofiles

from .core import TargetConfig, PhotonicTensor


class CachePolicy(Enum):
    """Cache eviction policies."""
    LRU = "lru"              # Least Recently Used
    LFU = "lfu"              # Least Frequently Used
    TTL = "ttl"              # Time To Live
    ADAPTIVE = "adaptive"     # Adaptive based on access patterns
    THERMAL_AWARE = "thermal_aware"  # Consider thermal impact


class CacheLevel(Enum):
    """Cache hierarchy levels."""
    L1_MEMORY = "l1_memory"        # Fast in-memory cache
    L2_COMPRESSED = "l2_compressed"  # Compressed in-memory cache
    L3_DISK = "l3_disk"            # Disk-based cache
    L4_DISTRIBUTED = "l4_distributed"  # Distributed cache across nodes


@dataclass
class CacheEntry:
    """Represents an entry in the cache."""
    key: str
    value: Any
    created_time: float
    last_accessed: float
    access_count: int = 0
    size_bytes: int = 0
    compression_level: int = 0
    thermal_cost: float = 0.0  # Thermal cost to regenerate
    dependencies: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def update_access(self):
        """Update access statistics."""
        self.last_accessed = time.time()
        self.access_count += 1
    
    def is_expired(self, ttl_seconds: float) -> bool:
        """Check if entry has expired."""
        return time.time() - self.created_time > ttl_seconds
    
    def get_age_seconds(self) -> float:
        """Get age of entry in seconds."""
        return time.time() - self.created_time


class CacheStatistics:
    """Tracks cache performance metrics."""
    
    def __init__(self):
        self.hits = 0
        self.misses = 0
        self.evictions = 0
        self.invalidations = 0
        self.total_size_bytes = 0
        self.compression_ratio = 0.0
        self.thermal_savings_mw = 0.0
        self.lock = threading.RLock()
    
    def record_hit(self, entry: CacheEntry):
        with self.lock:
            self.hits += 1
            self.thermal_savings_mw += entry.thermal_cost
    
    def record_miss(self):
        with self.lock:
            self.misses += 1
    
    def record_eviction(self, entry: CacheEntry):
        with self.lock:
            self.evictions += 1
            self.total_size_bytes -= entry.size_bytes
    
    def record_insertion(self, entry: CacheEntry):
        with self.lock:
            self.total_size_bytes += entry.size_bytes
    
    def get_hit_rate(self) -> float:
        with self.lock:
            total = self.hits + self.misses
            return self.hits / total if total > 0 else 0.0
    
    def get_metrics(self) -> Dict[str, Any]:
        with self.lock:
            return {
                "hits": self.hits,
                "misses": self.misses,
                "hit_rate": self.get_hit_rate(),
                "evictions": self.evictions,
                "invalidations": self.invalidations,
                "total_size_bytes": self.total_size_bytes,
                "compression_ratio": self.compression_ratio,
                "thermal_savings_mw": self.thermal_savings_mw
            }


class CacheBackend(ABC):
    """Abstract base class for cache backends."""
    
    @abstractmethod
    def get(self, key: str) -> Optional[CacheEntry]:
        """Get entry from cache."""
        pass
    
    @abstractmethod
    def put(self, key: str, entry: CacheEntry) -> bool:
        """Put entry into cache."""
        pass
    
    @abstractmethod
    def delete(self, key: str) -> bool:
        """Delete entry from cache."""
        pass
    
    @abstractmethod
    def clear(self) -> int:
        """Clear all entries and return count."""
        pass
    
    @abstractmethod
    def size(self) -> int:
        """Get number of entries."""
        pass


class MemoryCacheBackend(CacheBackend):
    """In-memory cache backend with configurable eviction policy."""
    
    def __init__(self, max_size: int = 1000, max_memory_mb: float = 512.0,\n                 policy: CachePolicy = CachePolicy.ADAPTIVE):\n        self.max_size = max_size\n        self.max_memory_bytes = int(max_memory_mb * 1024 * 1024)\n        self.policy = policy\n        \n        self.cache: OrderedDict[str, CacheEntry] = OrderedDict()\n        self.access_times: Dict[str, List[float]] = defaultdict(list)\n        self.lock = threading.RLock()\n        \n        self.current_memory_bytes = 0\n        self.logger = logging.getLogger(f\"{__name__}.MemoryCacheBackend\")\n    \n    def get(self, key: str) -> Optional[CacheEntry]:\n        with self.lock:\n            if key in self.cache:\n                entry = self.cache[key]\n                entry.update_access()\n                \n                # Move to end for LRU\n                if self.policy == CachePolicy.LRU:\n                    self.cache.move_to_end(key)\n                \n                # Record access time for adaptive policy\n                self.access_times[key].append(time.time())\n                if len(self.access_times[key]) > 10:\n                    self.access_times[key] = self.access_times[key][-5:]\n                \n                return entry\n            \n            return None\n    \n    def put(self, key: str, entry: CacheEntry) -> bool:\n        with self.lock:\n            # Check if we need to evict entries\n            while (len(self.cache) >= self.max_size or \n                   self.current_memory_bytes + entry.size_bytes > self.max_memory_bytes):\n                \n                if not self._evict_entry():\n                    return False  # Could not evict any entries\n            \n            # Remove existing entry if present\n            if key in self.cache:\n                old_entry = self.cache[key]\n                self.current_memory_bytes -= old_entry.size_bytes\n            \n            # Add new entry\n            self.cache[key] = entry\n            self.current_memory_bytes += entry.size_bytes\n            \n            self.logger.debug(f\"Cached entry {key}, size: {entry.size_bytes} bytes\")\n            return True\n    \n    def delete(self, key: str) -> bool:\n        with self.lock:\n            if key in self.cache:\n                entry = self.cache.pop(key)\n                self.current_memory_bytes -= entry.size_bytes\n                if key in self.access_times:\n                    del self.access_times[key]\n                return True\n            return False\n    \n    def clear(self) -> int:\n        with self.lock:\n            count = len(self.cache)\n            self.cache.clear()\n            self.access_times.clear()\n            self.current_memory_bytes = 0\n            return count\n    \n    def size(self) -> int:\n        with self.lock:\n            return len(self.cache)\n    \n    def _evict_entry(self) -> bool:\n        \"\"\"Evict an entry based on the configured policy.\"\"\"\n        if not self.cache:\n            return False\n        \n        if self.policy == CachePolicy.LRU:\n            # Remove least recently used (first item in OrderedDict)\n            key = next(iter(self.cache))\n        \n        elif self.policy == CachePolicy.LFU:\n            # Remove least frequently used\n            key = min(self.cache.keys(), key=lambda k: self.cache[k].access_count)\n        \n        elif self.policy == CachePolicy.TTL:\n            # Remove expired entries first, then oldest\n            expired_keys = [k for k, v in self.cache.items() if v.is_expired(3600.0)]\n            if expired_keys:\n                key = expired_keys[0]\n            else:\n                key = min(self.cache.keys(), key=lambda k: self.cache[k].created_time)\n        \n        elif self.policy == CachePolicy.THERMAL_AWARE:\n            # Remove entry with lowest thermal cost (cheapest to regenerate)\n            key = min(self.cache.keys(), key=lambda k: self.cache[k].thermal_cost)\n        \n        else:  # ADAPTIVE\n            key = self._adaptive_eviction()\n        \n        # Remove the selected entry\n        entry = self.cache.pop(key)\n        self.current_memory_bytes -= entry.size_bytes\n        if key in self.access_times:\n            del self.access_times[key]\n        \n        self.logger.debug(f\"Evicted entry {key} using {self.policy.value} policy\")\n        return True\n    \n    def _adaptive_eviction(self) -> str:\n        \"\"\"Adaptive eviction based on access patterns and thermal cost.\"\"\"\n        scores = {}\n        current_time = time.time()\n        \n        for key, entry in self.cache.items():\n            # Calculate composite score\n            age_factor = (current_time - entry.last_accessed) / 3600.0  # Hours\n            frequency_factor = 1.0 / (entry.access_count + 1)\n            thermal_factor = 1.0 / (entry.thermal_cost + 1.0)\n            size_factor = entry.size_bytes / (1024 * 1024)  # MB\n            \n            # Recent access pattern\n            recent_accesses = self.access_times.get(key, [])\n            if recent_accesses and len(recent_accesses) > 1:\n                access_rate = len(recent_accesses) / (current_time - recent_accesses[0] + 1)\n                rate_factor = 1.0 / (access_rate + 0.1)\n            else:\n                rate_factor = 1.0\n            \n            # Higher score = better candidate for eviction\n            scores[key] = age_factor * frequency_factor * thermal_factor * size_factor * rate_factor\n        \n        return max(scores.keys(), key=lambda k: scores[k])\n\n\nclass CompressedCacheBackend(CacheBackend):\n    \"\"\"Cache backend with transparent compression.\"\"\"\n    \n    def __init__(self, base_backend: CacheBackend, compression_level: int = 6):\n        self.base_backend = base_backend\n        self.compression_level = compression_level\n        self.logger = logging.getLogger(f\"{__name__}.CompressedCacheBackend\")\n    \n    def get(self, key: str) -> Optional[CacheEntry]:\n        entry = self.base_backend.get(key)\n        if entry and entry.compression_level > 0:\n            # Decompress the value\n            try:\n                decompressed_data = zlib.decompress(entry.value)\n                entry.value = pickle.loads(decompressed_data)\n                entry.compression_level = 0  # Mark as decompressed\n            except Exception as e:\n                self.logger.error(f\"Decompression failed for key {key}: {e}\")\n                return None\n        \n        return entry\n    \n    def put(self, key: str, entry: CacheEntry) -> bool:\n        # Compress the value if not already compressed\n        if entry.compression_level == 0:\n            try:\n                serialized_data = pickle.dumps(entry.value)\n                compressed_data = zlib.compress(serialized_data, self.compression_level)\n                \n                # Update entry with compressed data\n                original_size = entry.size_bytes or len(serialized_data)\n                entry.value = compressed_data\n                entry.size_bytes = len(compressed_data)\n                entry.compression_level = self.compression_level\n                \n                compression_ratio = original_size / entry.size_bytes if entry.size_bytes > 0 else 1.0\n                self.logger.debug(f\"Compressed entry {key}: {original_size} -> {entry.size_bytes} bytes \"\n                                f\"(ratio: {compression_ratio:.2f}x)\")\n                \n            except Exception as e:\n                self.logger.error(f\"Compression failed for key {key}: {e}\")\n                return False\n        \n        return self.base_backend.put(key, entry)\n    \n    def delete(self, key: str) -> bool:\n        return self.base_backend.delete(key)\n    \n    def clear(self) -> int:\n        return self.base_backend.clear()\n    \n    def size(self) -> int:\n        return self.base_backend.size()\n\n\nclass DiskCacheBackend(CacheBackend):\n    \"\"\"Persistent disk-based cache backend.\"\"\"\n    \n    def __init__(self, cache_dir: str, max_size_gb: float = 10.0):\n        self.cache_dir = Path(cache_dir)\n        self.cache_dir.mkdir(parents=True, exist_ok=True)\n        self.max_size_bytes = int(max_size_gb * 1024 * 1024 * 1024)\n        \n        self.index_file = self.cache_dir / \"cache_index.json\"\n        self.index: Dict[str, Dict[str, Any]] = self._load_index()\n        self.lock = threading.RLock()\n        \n        self.logger = logging.getLogger(f\"{__name__}.DiskCacheBackend\")\n    \n    def get(self, key: str) -> Optional[CacheEntry]:\n        with self.lock:\n            if key not in self.index:\n                return None\n            \n            try:\n                entry_info = self.index[key]\n                file_path = self.cache_dir / entry_info[\"filename\"]\n                \n                if not file_path.exists():\n                    # Clean up stale index entry\n                    del self.index[key]\n                    self._save_index()\n                    return None\n                \n                # Load and deserialize entry\n                with open(file_path, 'rb') as f:\n                    data = f.read()\n                \n                entry = pickle.loads(data)\n                entry.update_access()\n                \n                # Update index\n                self.index[key][\"last_accessed\"] = entry.last_accessed\n                self.index[key][\"access_count\"] = entry.access_count\n                \n                return entry\n                \n            except Exception as e:\n                self.logger.error(f\"Error loading cached entry {key}: {e}\")\n                # Clean up problematic entry\n                self.delete(key)\n                return None\n    \n    def put(self, key: str, entry: CacheEntry) -> bool:\n        with self.lock:\n            try:\n                # Clean up old entry if exists\n                if key in self.index:\n                    self.delete(key)\n                \n                # Serialize entry\n                data = pickle.dumps(entry)\n                \n                # Check size limits\n                if len(data) > self.max_size_bytes:\n                    self.logger.warning(f\"Entry {key} too large for disk cache: {len(data)} bytes\")\n                    return False\n                \n                # Ensure we have space\n                while self._get_total_size() + len(data) > self.max_size_bytes:\n                    if not self._evict_oldest():\n                        return False\n                \n                # Write to disk\n                filename = f\"entry_{hashlib.md5(key.encode()).hexdigest()}.cache\"\n                file_path = self.cache_dir / filename\n                \n                with open(file_path, 'wb') as f:\n                    f.write(data)\n                \n                # Update index\n                self.index[key] = {\n                    \"filename\": filename,\n                    \"size_bytes\": len(data),\n                    \"created_time\": entry.created_time,\n                    \"last_accessed\": entry.last_accessed,\n                    \"access_count\": entry.access_count\n                }\n                \n                self._save_index()\n                self.logger.debug(f\"Saved entry {key} to disk: {len(data)} bytes\")\n                return True\n                \n            except Exception as e:\n                self.logger.error(f\"Error saving entry {key} to disk: {e}\")\n                return False\n    \n    def delete(self, key: str) -> bool:\n        with self.lock:\n            if key not in self.index:\n                return False\n            \n            try:\n                entry_info = self.index[key]\n                file_path = self.cache_dir / entry_info[\"filename\"]\n                \n                if file_path.exists():\n                    file_path.unlink()\n                \n                del self.index[key]\n                self._save_index()\n                return True\n                \n            except Exception as e:\n                self.logger.error(f\"Error deleting entry {key}: {e}\")\n                return False\n    \n    def clear(self) -> int:\n        with self.lock:\n            count = len(self.index)\n            \n            # Delete all cache files\n            for entry_info in self.index.values():\n                try:\n                    file_path = self.cache_dir / entry_info[\"filename\"]\n                    if file_path.exists():\n                        file_path.unlink()\n                except Exception as e:\n                    self.logger.error(f\"Error deleting cache file: {e}\")\n            \n            self.index.clear()\n            self._save_index()\n            return count\n    \n    def size(self) -> int:\n        with self.lock:\n            return len(self.index)\n    \n    def _load_index(self) -> Dict[str, Dict[str, Any]]:\n        \"\"\"Load cache index from disk.\"\"\"\n        try:\n            if self.index_file.exists():\n                with open(self.index_file, 'r') as f:\n                    return json.load(f)\n        except Exception as e:\n            self.logger.error(f\"Error loading cache index: {e}\")\n        \n        return {}\n    \n    def _save_index(self):\n        \"\"\"Save cache index to disk.\"\"\"\n        try:\n            with open(self.index_file, 'w') as f:\n                json.dump(self.index, f, indent=2)\n        except Exception as e:\n            self.logger.error(f\"Error saving cache index: {e}\")\n    \n    def _get_total_size(self) -> int:\n        \"\"\"Get total size of cached data.\"\"\"\n        return sum(info[\"size_bytes\"] for info in self.index.values())\n    \n    def _evict_oldest(self) -> bool:\n        \"\"\"Evict the oldest cache entry.\"\"\"\n        if not self.index:\n            return False\n        \n        oldest_key = min(self.index.keys(), \n                        key=lambda k: self.index[k][\"last_accessed\"])\n        \n        return self.delete(oldest_key)\n\n\nclass HierarchicalCache:\n    \"\"\"\n    Multi-level hierarchical cache system.\n    \n    Implements L1 (memory) -> L2 (compressed memory) -> L3 (disk) cache hierarchy\n    with intelligent promotion and demotion based on access patterns.\n    \"\"\"\n    \n    def __init__(self, \n                 l1_size: int = 100,\n                 l1_memory_mb: float = 256.0,\n                 l2_size: int = 500,\n                 l2_memory_mb: float = 512.0,\n                 l3_size_gb: float = 5.0,\n                 cache_dir: str = \"./photonic_cache\"):\n        \n        # Create cache levels\n        self.l1_cache = MemoryCacheBackend(l1_size, l1_memory_mb, CachePolicy.ADAPTIVE)\n        \n        l2_base = MemoryCacheBackend(l2_size, l2_memory_mb, CachePolicy.LRU)\n        self.l2_cache = CompressedCacheBackend(l2_base)\n        \n        self.l3_cache = DiskCacheBackend(cache_dir, l3_size_gb)\n        \n        self.statistics = {\n            CacheLevel.L1_MEMORY: CacheStatistics(),\n            CacheLevel.L2_COMPRESSED: CacheStatistics(),\n            CacheLevel.L3_DISK: CacheStatistics()\n        }\n        \n        self.access_patterns: Dict[str, List[float]] = defaultdict(list)\n        self.promotion_threshold = 3  # Promote after 3 accesses\n        self.lock = threading.RLock()\n        \n        self.logger = logging.getLogger(f\"{__name__}.HierarchicalCache\")\n    \n    def get(self, key: str) -> Optional[Any]:\n        \"\"\"Get value from cache, checking L1 -> L2 -> L3.\"\"\"\n        with self.lock:\n            # Try L1 cache first\n            entry = self.l1_cache.get(key)\n            if entry:\n                self.statistics[CacheLevel.L1_MEMORY].record_hit(entry)\n                self._record_access(key)\n                return entry.value\n            \n            # Try L2 cache\n            entry = self.l2_cache.get(key)\n            if entry:\n                self.statistics[CacheLevel.L2_COMPRESSED].record_hit(entry)\n                self._record_access(key)\n                \n                # Promote to L1 if frequently accessed\n                if self._should_promote_to_l1(key):\n                    self._promote_to_l1(key, entry)\n                \n                return entry.value\n            \n            # Try L3 cache\n            entry = self.l3_cache.get(key)\n            if entry:\n                self.statistics[CacheLevel.L3_DISK].record_hit(entry)\n                self._record_access(key)\n                \n                # Promote to L2 if appropriate\n                if self._should_promote_to_l2(key):\n                    self._promote_to_l2(key, entry)\n                \n                return entry.value\n            \n            # Cache miss at all levels\n            for stats in self.statistics.values():\n                stats.record_miss()\n            \n            return None\n    \n    def put(self, key: str, value: Any, thermal_cost: float = 0.0, \n           dependencies: List[str] = None) -> bool:\n        \"\"\"Put value into appropriate cache level.\"\"\"\n        with self.lock:\n            # Create cache entry\n            entry = CacheEntry(\n                key=key,\n                value=value,\n                created_time=time.time(),\n                last_accessed=time.time(),\n                size_bytes=self._estimate_size(value),\n                thermal_cost=thermal_cost,\n                dependencies=dependencies or []\n            )\n            \n            # Determine initial cache level based on size and thermal cost\n            if entry.size_bytes < 1024 * 1024 and thermal_cost > 10.0:  # < 1MB, high thermal cost\n                # Put in L1 for fast access\n                success = self.l1_cache.put(key, entry)\n                if success:\n                    self.statistics[CacheLevel.L1_MEMORY].record_insertion(entry)\n                    return True\n            \n            # Try L2 cache\n            success = self.l2_cache.put(key, entry)\n            if success:\n                self.statistics[CacheLevel.L2_COMPRESSED].record_insertion(entry)\n                return True\n            \n            # Fall back to L3 cache\n            success = self.l3_cache.put(key, entry)\n            if success:\n                self.statistics[CacheLevel.L3_DISK].record_insertion(entry)\n                return True\n            \n            self.logger.warning(f\"Failed to cache entry {key} at any level\")\n            return False\n    \n    def invalidate(self, key: str) -> bool:\n        \"\"\"Invalidate entry from all cache levels.\"\"\"\n        with self.lock:\n            invalidated = False\n            \n            if self.l1_cache.delete(key):\n                invalidated = True\n            \n            if self.l2_cache.delete(key):\n                invalidated = True\n            \n            if self.l3_cache.delete(key):\n                invalidated = True\n            \n            if key in self.access_patterns:\n                del self.access_patterns[key]\n            \n            if invalidated:\n                for stats in self.statistics.values():\n                    stats.invalidations += 1\n            \n            return invalidated\n    \n    def invalidate_dependencies(self, dependency: str) -> int:\n        \"\"\"Invalidate all entries that depend on a given dependency.\"\"\"\n        invalidated_count = 0\n        \n        # Check all cache levels for dependent entries\n        for cache in [self.l1_cache, self.l2_cache, self.l3_cache]:\n            keys_to_invalidate = []\n            \n            # Scan for dependent entries (this is simplified - in practice would need better indexing)\n            for i in range(cache.size()):\n                # This would need proper implementation based on cache backend\n                pass\n        \n        return invalidated_count\n    \n    def clear(self) -> Dict[str, int]:\n        \"\"\"Clear all cache levels.\"\"\"\n        with self.lock:\n            counts = {\n                \"l1\": self.l1_cache.clear(),\n                \"l2\": self.l2_cache.clear(),\n                \"l3\": self.l3_cache.clear()\n            }\n            \n            self.access_patterns.clear()\n            \n            # Reset statistics\n            for stats in self.statistics.values():\n                stats.__init__()\n            \n            return counts\n    \n    def get_statistics(self) -> Dict[str, Dict[str, Any]]:\n        \"\"\"Get comprehensive cache statistics.\"\"\"\n        with self.lock:\n            return {\n                \"l1_memory\": self.statistics[CacheLevel.L1_MEMORY].get_metrics(),\n                \"l2_compressed\": self.statistics[CacheLevel.L2_COMPRESSED].get_metrics(),\n                \"l3_disk\": self.statistics[CacheLevel.L3_DISK].get_metrics(),\n                \"overall\": self._calculate_overall_metrics()\n            }\n    \n    def optimize_cache(self):\n        \"\"\"Optimize cache performance based on access patterns.\"\"\"\n        with self.lock:\n            # Analyze access patterns and promote/demote entries\n            current_time = time.time()\n            \n            for key, accesses in list(self.access_patterns.items()):\n                if not accesses:\n                    continue\n                \n                # Clean old access records\n                recent_accesses = [a for a in accesses if current_time - a < 3600]  # Last hour\n                self.access_patterns[key] = recent_accesses[-10:]  # Keep last 10\n                \n                if len(recent_accesses) >= self.promotion_threshold:\n                    # Try to promote frequently accessed items\n                    self._try_promote(key)\n    \n    def _record_access(self, key: str):\n        \"\"\"Record access time for promotion/demotion decisions.\"\"\"\n        self.access_patterns[key].append(time.time())\n        if len(self.access_patterns[key]) > 20:  # Keep history manageable\n            self.access_patterns[key] = self.access_patterns[key][-10:]\n    \n    def _should_promote_to_l1(self, key: str) -> bool:\n        \"\"\"Determine if entry should be promoted to L1 cache.\"\"\"\n        accesses = self.access_patterns.get(key, [])\n        if len(accesses) < 2:\n            return False\n        \n        # Check access frequency\n        recent_accesses = [a for a in accesses if time.time() - a < 300]  # Last 5 minutes\n        return len(recent_accesses) >= 2\n    \n    def _should_promote_to_l2(self, key: str) -> bool:\n        \"\"\"Determine if entry should be promoted to L2 cache.\"\"\"\n        accesses = self.access_patterns.get(key, [])\n        if len(accesses) < self.promotion_threshold:\n            return False\n        \n        # Check recent access pattern\n        current_time = time.time()\n        recent_accesses = [a for a in accesses if current_time - a < 1800]  # Last 30 minutes\n        return len(recent_accesses) >= 2\n    \n    def _promote_to_l1(self, key: str, entry: CacheEntry):\n        \"\"\"Promote entry to L1 cache.\"\"\"\n        if self.l1_cache.put(key, entry):\n            self.statistics[CacheLevel.L1_MEMORY].record_insertion(entry)\n            self.logger.debug(f\"Promoted {key} to L1 cache\")\n    \n    def _promote_to_l2(self, key: str, entry: CacheEntry):\n        \"\"\"Promote entry to L2 cache.\"\"\"\n        if self.l2_cache.put(key, entry):\n            self.statistics[CacheLevel.L2_COMPRESSED].record_insertion(entry)\n            self.logger.debug(f\"Promoted {key} to L2 cache\")\n    \n    def _try_promote(self, key: str):\n        \"\"\"Try to promote an entry to a higher cache level.\"\"\"\n        # Check if entry exists in L3 and should be promoted\n        entry = self.l3_cache.get(key)\n        if entry and self._should_promote_to_l2(key):\n            self._promote_to_l2(key, entry)\n            return\n        \n        # Check if entry exists in L2 and should be promoted to L1\n        entry = self.l2_cache.get(key)\n        if entry and self._should_promote_to_l1(key):\n            self._promote_to_l1(key, entry)\n    \n    def _estimate_size(self, value: Any) -> int:\n        \"\"\"Estimate size of value in bytes.\"\"\"\n        try:\n            return len(pickle.dumps(value))\n        except:\n            return 1024  # Default estimate\n    \n    def _calculate_overall_metrics(self) -> Dict[str, Any]:\n        \"\"\"Calculate overall cache metrics across all levels.\"\"\"\n        total_hits = sum(stats.hits for stats in self.statistics.values())\n        total_misses = sum(stats.misses for stats in self.statistics.values())\n        total_size = sum(stats.total_size_bytes for stats in self.statistics.values())\n        total_thermal_savings = sum(stats.thermal_savings_mw for stats in self.statistics.values())\n        \n        return {\n            \"total_hits\": total_hits,\n            \"total_misses\": total_misses,\n            \"overall_hit_rate\": total_hits / (total_hits + total_misses) if (total_hits + total_misses) > 0 else 0.0,\n            \"total_size_bytes\": total_size,\n            \"total_thermal_savings_mw\": total_thermal_savings,\n            \"cache_levels_active\": len(self.statistics)\n        }\n\n\n# Specialized caches for photonic compilation\nclass PhotonicCompilationCache:\n    \"\"\"\n    Specialized cache for photonic compilation results.\n    \n    Provides domain-specific caching with intelligent invalidation\n    based on model changes and compilation parameters.\n    \"\"\"\n    \n    def __init__(self, cache_dir: str = \"./photonic_cache\"):\n        self.cache = HierarchicalCache(cache_dir=cache_dir)\n        self.dependency_graph: Dict[str, List[str]] = defaultdict(list)\n        self.model_hashes: Dict[str, str] = {}\n        self.config_cache: Dict[str, str] = {}\n        self.lock = threading.RLock()\n        \n        self.logger = logging.getLogger(f\"{__name__}.PhotonicCompilationCache\")\n    \n    def get_compiled_model(self, model_path: str, config: TargetConfig) -> Optional[Any]:\n        \"\"\"Get cached compilation result.\"\"\"\n        cache_key = self._generate_cache_key(model_path, config)\n        \n        # Check if model or config has changed\n        if self._is_cache_invalid(model_path, config, cache_key):\n            self.cache.invalidate(cache_key)\n            return None\n        \n        return self.cache.get(cache_key)\n    \n    def cache_compiled_model(self, model_path: str, config: TargetConfig, \n                           compiled_result: Any, thermal_cost: float = 50.0) -> bool:\n        \"\"\"Cache compilation result.\"\"\"\n        cache_key = self._generate_cache_key(model_path, config)\n        \n        # Update dependency tracking\n        self._update_dependencies(model_path, cache_key)\n        \n        # Store model hash and config hash\n        self.model_hashes[model_path] = self._calculate_file_hash(model_path)\n        self.config_cache[cache_key] = self._calculate_config_hash(config)\n        \n        return self.cache.put(cache_key, compiled_result, thermal_cost)\n    \n    def invalidate_model_cache(self, model_path: str) -> int:\n        \"\"\"Invalidate all cached results for a model.\"\"\"\n        model_hash = self._calculate_file_hash(model_path)\n        invalidated = 0\n        \n        # Find all cache keys related to this model\n        keys_to_invalidate = [\n            key for key in self.model_hashes.keys() \n            if key.startswith(f\"model:{model_hash}:\")\n        ]\n        \n        for key in keys_to_invalidate:\n            if self.cache.invalidate(key):\n                invalidated += 1\n        \n        return invalidated\n    \n    def get_cache_statistics(self) -> Dict[str, Any]:\n        \"\"\"Get cache statistics with photonic-specific metrics.\"\"\"\n        stats = self.cache.get_statistics()\n        \n        # Add domain-specific metrics\n        stats[\"photonic_specific\"] = {\n            \"cached_models\": len(self.model_hashes),\n            \"dependency_graph_size\": len(self.dependency_graph),\n            \"config_variations\": len(self.config_cache)\n        }\n        \n        return stats\n    \n    def _generate_cache_key(self, model_path: str, config: TargetConfig) -> str:\n        \"\"\"Generate unique cache key for model and configuration.\"\"\"\n        model_hash = self._calculate_file_hash(model_path)\n        config_hash = self._calculate_config_hash(config)\n        return f\"model:{model_hash}:config:{config_hash}\"\n    \n    def _calculate_file_hash(self, file_path: str) -> str:\n        \"\"\"Calculate hash of file contents.\"\"\"\n        try:\n            with open(file_path, 'rb') as f:\n                return hashlib.sha256(f.read()).hexdigest()[:16]\n        except Exception:\n            return hashlib.sha256(file_path.encode()).hexdigest()[:16]\n    \n    def _calculate_config_hash(self, config: TargetConfig) -> str:\n        \"\"\"Calculate hash of configuration.\"\"\"\n        config_str = json.dumps(config.to_dict(), sort_keys=True)\n        return hashlib.sha256(config_str.encode()).hexdigest()[:16]\n    \n    def _is_cache_invalid(self, model_path: str, config: TargetConfig, cache_key: str) -> bool:\n        \"\"\"Check if cache entry is invalid due to changes.\"\"\"\n        # Check model file changes\n        current_hash = self._calculate_file_hash(model_path)\n        if model_path in self.model_hashes:\n            if self.model_hashes[model_path] != current_hash:\n                return True\n        \n        # Check configuration changes\n        current_config_hash = self._calculate_config_hash(config)\n        if cache_key in self.config_cache:\n            if self.config_cache[cache_key] != current_config_hash:\n                return True\n        \n        return False\n    \n    def _update_dependencies(self, model_path: str, cache_key: str):\n        \"\"\"Update dependency graph for invalidation.\"\"\"\n        with self.lock:\n            if model_path not in self.dependency_graph:\n                self.dependency_graph[model_path] = []\n            \n            if cache_key not in self.dependency_graph[model_path]:\n                self.dependency_graph[model_path].append(cache_key)\n\n\n# Global cache instance\n_global_cache: Optional[PhotonicCompilationCache] = None\n\n\ndef get_global_cache() -> PhotonicCompilationCache:\n    \"\"\"Get global photonic compilation cache instance.\"\"\"\n    global _global_cache\n    if _global_cache is None:\n        _global_cache = PhotonicCompilationCache()\n    return _global_cache\n\n\ndef clear_global_cache() -> Dict[str, int]:\n    \"\"\"Clear the global cache.\"\"\"\n    cache = get_global_cache()\n    return cache.cache.clear()\n\n\n# Decorator for automatic caching\ndef cached_compilation(thermal_cost: float = 50.0):\n    \"\"\"Decorator for automatic caching of compilation functions.\"\"\"\n    def decorator(func: Callable) -> Callable:\n        def wrapper(model_path: str, config: TargetConfig, *args, **kwargs):\n            cache = get_global_cache()\n            \n            # Try to get from cache\n            result = cache.get_compiled_model(model_path, config)\n            if result is not None:\n                return result\n            \n            # Compute and cache result\n            result = func(model_path, config, *args, **kwargs)\n            cache.cache_compiled_model(model_path, config, result, thermal_cost)\n            \n            return result\n        \n        return wrapper\n    return decorator"